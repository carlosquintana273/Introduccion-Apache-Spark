{"cells": [{"cell_type": "markdown", "id": "527347c2", "metadata": {}, "source": "# Poblando Capa Landing\n\n### Poblando Clientes\n\n**1\u00b0 PASO:** Importamos m\u00f3dulos de apache spark"}, {"cell_type": "code", "execution_count": 46, "id": "df78e2fb", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *"}, {"cell_type": "markdown", "id": "a0f15755", "metadata": {}, "source": "**2\u00b0 PASO:** Creamos las session de apache spark en una variable"}, {"cell_type": "code", "execution_count": 47, "id": "557b635e", "metadata": {}, "outputs": [], "source": "spark = SparkSession.builder.getOrCreate()"}, {"cell_type": "markdown", "id": "0a1df9d8", "metadata": {}, "source": "**3\u00b0 PASO:** Verificamos la versi\u00f3n de apache spark"}, {"cell_type": "code", "execution_count": 48, "id": "c84a3cb0", "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cluster-6277-m.c.course-big-data-336218.internal:36417\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f67fb40d430>"}, "execution_count": 48, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "markdown", "id": "df8c09a3", "metadata": {}, "source": "**4\u00b0 PASO:** Crear un dataframe\n\n1.   Crear la estructura del dataframe\n2.   Declarar en una variable la ruta del archivo\n3.   Leer el archivo de origen\n4.   Mostrar la estructura del dataframe\n5.   mostrar los datos del dataframe\n6.   Cantidad de registros del dataframe\n7.   Mostrar las estad\u00edsticas b\u00e1sicas de un campo determinado"}, {"cell_type": "code", "execution_count": 49, "id": "813641e7", "metadata": {}, "outputs": [], "source": "# 4.1 Estructura del dataframe.\ndf_schema = StructType([\nStructField(\"ID\", StringType(),True),\nStructField(\"NOMBRE\", StringType(),True),\nStructField(\"TELEFONO\", StringType(),True),\nStructField(\"CORREO\", StringType(),True),\nStructField(\"FECHA_INGRESO\", StringType(),True),\nStructField(\"EDAD\", IntegerType(),True),\nStructField(\"SALARIO\", DoubleType(),True),\nStructField(\"ID_EMPRESA\", StringType(),True),\n])"}, {"cell_type": "code", "execution_count": 50, "id": "af4cb31b", "metadata": {}, "outputs": [], "source": "# 4.2 Definimos ruta del archivo\n\n#Archivo en Cloud Storage - Google Cloud Platform\nruta_persona_raw = \"gs://curso-apache-spark/datalake/workload/personas/persona.data\"\n\n#Archivo DBFS - DataBricks\n# ruta_persona_raw = \"/FileStore/tables/persona.data\"\n\n#Archivo en HDFS - Hadoop\n#ruta_persona_raw = \"hdfs:/datalake/workload/personas/persona.data\""}, {"cell_type": "code", "execution_count": 51, "id": "ee4999a0", "metadata": {}, "outputs": [], "source": "#Leer el archivo de origen\ndf_personas = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"delimiter\",\"|\").schema(df_schema).load(ruta_persona_raw)"}, {"cell_type": "code", "execution_count": 52, "id": "9c5f6a57", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- ID: string (nullable = true)\n |-- NOMBRE: string (nullable = true)\n |-- TELEFONO: string (nullable = true)\n |-- CORREO: string (nullable = true)\n |-- FECHA_INGRESO: string (nullable = true)\n |-- EDAD: integer (nullable = true)\n |-- SALARIO: double (nullable = true)\n |-- ID_EMPRESA: string (nullable = true)\n\n"}], "source": "#4.4 Mostramos la estructura del dataframe.\ndf_personas.printSchema()"}, {"cell_type": "code", "execution_count": 53, "id": "c4ab6774", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "\r[Stage 17:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---+---------+--------------+--------------------+-------------+----+-------+----------+\n| ID|   NOMBRE|      TELEFONO|              CORREO|FECHA_INGRESO|EDAD|SALARIO|ID_EMPRESA|\n+---+---------+--------------+--------------------+-------------+----+-------+----------+\n|  1|     Carl|1-745-633-9145|arcu.Sed.et@ante....|   2004-04-23|  32|20095.0|         5|\n|  2|Priscilla|      155-2498|Donec.egestas.Ali...|   2019-02-17|  34| 9298.0|         2|\n|  3|  Jocelyn|1-204-956-8594|amet.diam@loborti...|   2002-08-01|  27|10853.0|         3|\n|  4|    Aidan|1-719-862-9385|euismod.et.commod...|   2018-11-06|  29| 3387.0|        10|\n|  5|  Leandra|      839-8044|at@pretiumetrutru...|   2002-10-10|  41|22102.0|         1|\n+---+---------+--------------+--------------------+-------------+----+-------+----------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "\r                                                                                \r"}], "source": "# 4.5 Mostraremos todos los datos del dataframe.\ndf_personas.show(5)"}, {"cell_type": "code", "execution_count": 54, "id": "9a425745", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "La cantidad de registro del dataframe es:  100\n"}], "source": "# 4.6 Mostraremos todos los datos del dataframe.\nnum_rows = df_personas.count()\n\nprint(\"La cantidad de registro del dataframe es: \", num_rows)"}, {"cell_type": "code", "execution_count": 55, "id": "f9df35e7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+-----------------+\n|summary|          salario|\n+-------+-----------------+\n|  count|              100|\n|   mean|         11684.55|\n| stddev|6841.493958437246|\n|    min|           1256.0|\n|    max|          24575.0|\n+-------+-----------------+\n\n"}], "source": "# 4.7 Estad\u00edsticas de un campo determinado.\ndf_personas.describe('salario').show()"}, {"cell_type": "markdown", "id": "b75152d1", "metadata": {}, "source": "**5\u00b0 PASO:** Guardar el dataframe en un ruta de la capa landing"}, {"cell_type": "code", "execution_count": 56, "id": "13d2a06c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "\n#Archivo en Cloud Storage - Google Cloud Platform\nruta_persona_landing = \"gs://curso-apache-spark/datalake/landing/personas/\"\n\n#Archivo DBFS - DataBricks\n# ruta_persona_landing = \"/FileStore/tables/landing/personas/\"\n\n\n#Archivo en HDFS - Hadoop\n#ruta_persona_landing = \"hdfs:/introduccion-apache-spark/datalake/landing/personas/\"\n\ndf_personas.write.mode(\"overwrite\").format(\"parquet\").save(ruta_persona_landing)"}, {"cell_type": "code", "execution_count": 57, "id": "948020a0", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "=========================================\nTermino procesamiento de Persona Landing\n=========================================\n \n=========================================\nInicio procesamiento de Empresa Landing\n=========================================\n"}], "source": "print(\"=========================================\")\nprint(\"Termino procesamiento de Persona Landing\")\nprint(\"=========================================\")\nprint(\" \")\nprint(\"=========================================\")\nprint(\"Inicio procesamiento de Empresa Landing\")\nprint(\"=========================================\")"}, {"cell_type": "markdown", "id": "f9cc1077", "metadata": {}, "source": "## Poblando Empresa\n**6\u00b0 PASO:** Realizar la ingesta de Empresas en la capa landing\n* Crear estructura del dataframe.\n* Definir la ruta del archivo.\n* Crear la estructura del dataframe\n* Declarar en una variable la ruta del archivo\n* Leer el archivo de origen\n* Mostrar la informaci\u00f3n del dataframe\n* Guardar el dataframe en un ruta de la capa landing"}, {"cell_type": "code", "execution_count": 58, "id": "3d5e0715", "metadata": {}, "outputs": [], "source": "# 6.1 Estructura del dataframe.\ndf_schema_empresas = StructType([\nStructField(\"ID\", StringType(),True),\nStructField(\"EMPRESA_NAME\", StringType(),True)\n])"}, {"cell_type": "code", "execution_count": 59, "id": "61eb19d4", "metadata": {}, "outputs": [], "source": "# 6.2 Definimos ruta del archivo\n\n#Archivo en Cloud Storage - Google Cloud Platform\nruta_empresa_raw = \"gs://curso-apache-spark/datalake/workload/empresas/empresa.data\"\n\n#Archivo DBFS - DataBricks\n# ruta_empresa_raw = \"/FileStore/tables/persona.data\"\n\n#Archivo en HDFS - Hadoop\n#ruta_empresa_raw = \"hdfs:/datalake/workload/personas/persona.data\""}, {"cell_type": "code", "execution_count": 60, "id": "9b1d27f8", "metadata": {}, "outputs": [], "source": "# 6.3 Creamos el dataframe \ndf_empresas = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"delimiter\",\"|\").schema(df_schema_empresas).load(ruta_empresa_raw)"}, {"cell_type": "code", "execution_count": 61, "id": "5188c92d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+------------+\n| ID|EMPRESA_NAME|\n+---+------------+\n|  1|     Walmart|\n|  2|   Microsoft|\n|  3|       Apple|\n|  4|      Toyota|\n|  5|      Amazon|\n|  6|      Google|\n|  7|     Samsung|\n|  8|          HP|\n|  9|         IBM|\n| 10|        Sony|\n+---+------------+\n\n"}], "source": "df_empresas.show(10)"}, {"cell_type": "code", "execution_count": 62, "id": "3ed0cdb3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# 6.5 Definimos la ruta de almacenamiento\n\n#Archivo en Cloud Storage - Google Cloud Platform\nruta_empresa_landing = \"gs://curso-apache-spark/datalake/landing/empresas/\"\n\n#Archivo DBFS - DataBricks\n# ruta_empresas_landing = \"/FileStore/tables/landing/empresas/\"\n\n\n#Archivo en HDFS - Hadoop\n#ruta_empresas_landing = \"hdfs:/introduccion-apache-spark/datalake/landing/empresas/\"\n\n\n# 6.6 Guardamos el archivo en formato parquet\ndf_empresas.write.mode(\"overwrite\").format(\"parquet\").save(ruta_empresa_landing)"}, {"cell_type": "code", "execution_count": 63, "id": "a1b6c9f6", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "=========================================\nTermino procesamiento de Empresa Landing\n=========================================\n \n=========================================\nInicio procesamiento de Transacciones Landing\n=========================================\n"}], "source": "print(\"=========================================\")\nprint(\"Termino procesamiento de Empresa Landing\")\nprint(\"=========================================\")\nprint(\" \")\nprint(\"=========================================\")\nprint(\"Inicio procesamiento de Transacciones Landing\")\nprint(\"=========================================\")"}, {"cell_type": "markdown", "id": "1c65fe70", "metadata": {}, "source": "## Poblando Transacciones\n**7\u00b0 PASO:** Realizar la ingesta de transacciones en la capa landing\n* Crear estructura del dataframe.\n* Definir la ruta del archivo.\n* Crear la estructura del dataframe\n* Declarar en una variable la ruta del archivo\n* Leer el archivo de origen\n* Mostrar la informaci\u00f3n del dataframe\n* Guardar el dataframe en un ruta de la capa landing"}, {"cell_type": "code", "execution_count": 64, "id": "b4b87bab", "metadata": {}, "outputs": [], "source": "# 6.1 Estructura del dataframe.\ndf_schema_transacciones = StructType([\nStructField(\"ID_PERSONA\", StringType(),True),\nStructField(\"ID_EMPRESA\", StringType(),True),\nStructField(\"MONTO\", DoubleType(),True),\nStructField(\"FECHA\", StringType(),True)\n])"}, {"cell_type": "code", "execution_count": 65, "id": "3310d17c", "metadata": {}, "outputs": [], "source": "# 4.2 Definimos ruta del archivo\n\n#Archivo en Cloud Storage - Google Cloud Platform\nruta_transacciones_raw = \"gs://curso-apache-spark/datalake/workload/transaccion/transacciones.data\"\n\n#Archivo DBFS - DataBricks\n# ruta_transacciones_raw = \"/FileStore/tables/transacciones.data\"\n\n#Archivo en HDFS - Hadoop\n#ruta_transacciones_raw = \"hdfs:/datalake/workload/transaccion/transacciones.data\""}, {"cell_type": "code", "execution_count": 66, "id": "aa5357b1", "metadata": {}, "outputs": [], "source": "df_transacciones = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"delimiter\",\"|\").schema(df_schema_transacciones).load(ruta_transacciones_raw)"}, {"cell_type": "code", "execution_count": 67, "id": "2e81b17a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+----------+------+----------+\n|ID_PERSONA|ID_EMPRESA| MONTO|     FECHA|\n+----------+----------+------+----------+\n|        18|         3|1383.0|2018-01-21|\n|        30|         6|2331.0|2018-01-21|\n|        47|         2|2280.0|2018-01-21|\n|        28|         1| 730.0|2018-01-21|\n|        91|         4|3081.0|2018-01-21|\n|        74|         8|2409.0|2018-01-21|\n|        41|         2|3754.0|2018-01-22|\n|        42|         9|4079.0|2018-01-22|\n|        24|         6|4475.0|2018-01-22|\n|        67|         9| 561.0|2018-01-22|\n|         9|         4|3765.0|2018-01-22|\n|        97|         3|3669.0|2018-01-22|\n|        91|         5|3497.0|2018-01-22|\n|        61|         3| 735.0|2018-01-23|\n|        15|         5| 367.0|2018-01-23|\n|        20|         9|2039.0|2018-01-23|\n|        11|         4| 719.0|2018-01-23|\n|        36|         2|2659.0|2018-01-23|\n|        12|         4| 467.0|2018-01-23|\n|        38|         9|2411.0|2018-01-23|\n+----------+----------+------+----------+\nonly showing top 20 rows\n\n"}], "source": "df_transacciones.show()"}, {"cell_type": "code", "execution_count": null, "id": "95cc0945", "metadata": {}, "outputs": [], "source": "# 7.5 Definimos la ruta de almacenamiento\n\n#Archivo en Cloud Storage - Google Cloud Platform\nruta_transacciones_landing = \"gs://curso-apache-spark/datalake/landing/transacciones/\"\n\n#Archivo DBFS - DataBricks\n# ruta_transacciones_landing = \"/FileStore/tables/landing/transacciones/\"\n\n\n#Archivo en HDFS - Hadoop\n#ruta_transacciones_landing = \"hdfs:/introduccion-apache-spark/datalake/landing/empresas/\"\n\n\n# 6.6 Guardamos el archivo en formato parquet\ndf_transacciones.write.mode(\"overwrite\").format(\"parquet\").save(ruta_transacciones_landing)"}, {"cell_type": "code", "execution_count": null, "id": "b0c3da8f", "metadata": {}, "outputs": [], "source": "print(\"=========================================\")\nprint(\"Termino procesamiento de Transacciones Landing\")\nprint(\"=========================================\")"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 5}